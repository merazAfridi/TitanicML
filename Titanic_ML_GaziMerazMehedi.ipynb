{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM2tuD9EWMNpPhMDKdj4rpl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/merazAfridi/TitanicML/blob/main/Titanic_ML_GaziMerazMehedi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "In this notebook I will dive into the Titanic dataset, and see if I can predict; with high accuracy , if a passenger survived or not.\n",
        "\n",
        "I will be incorporating machine learning, as well as feature engineering to build my models.\n",
        "\n",
        "This is for the ongoing Titanic - Machine Learning from Disaster competition."
      ],
      "metadata": {
        "id": "yMfk1MGWAtxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing Libraries**"
      ],
      "metadata": {
        "id": "ru-k9OzNA6Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For data manipulation and visualization\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For Machine Learning model building\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "\n",
        "# For model evaluation\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score, plot_confusion_matrix\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "EvU44lIfPejr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "fdd33060-6276-4bf6-b096-14a9d9f0da08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5f8b0c865b6d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# For model evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (/usr/local/lib/python3.10/dist-packages/sklearn/metrics/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process the data**"
      ],
      "metadata": {
        "id": "bLwhg0CKAJjp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I have browsed Titanic dataset from (https://www.kaggle.com/c/titanic/data) . Then Downloaded 'train.csv' and 'test.csv'.**"
      ],
      "metadata": {
        "id": "HI38AmuPCvOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv('/train.csv')\n",
        "test_df = pd.read_csv('/test.csv')"
      ],
      "metadata": {
        "id": "GvBeTyyGQ_1j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "74ded0c5-2cb9-446e-cbfe-bd2a9904d054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-0d8d7e5f8ad7>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YHGm0hJtDAMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Viewing the Data Information**"
      ],
      "metadata": {
        "id": "iEVRYha0BQ4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Train Data Shape: {train_df.shape}')\n",
        "print(f'Test Data Shape: {test_df.shape}')"
      ],
      "metadata": {
        "id": "pIz3ieM4RcGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets check the first five rows of Train data\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "up-5xgxMRnVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning the Data**"
      ],
      "metadata": {
        "id": "__CiuGg8Bedh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Droping PassengerId, Name and Ticket, I don't think its useful for model performance.***"
      ],
      "metadata": {
        "id": "6v2EudrQbOoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "OYM-B0e0bQUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summary Statistics\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "yRbJ0mKrbg-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values in train data\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "JC7BoKtAcBot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for null values in test data\n",
        "test_df.isnull().sum()"
      ],
      "metadata": {
        "id": "LwfIV5HjcFOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Most of the Cabin data is missing so I have decided to drop Cabin.***"
      ],
      "metadata": {
        "id": "98VNXlzScVJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop('Cabin', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "x_XS5PyUcYK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get an overview of features datatype\n",
        "train_df.dtypes"
      ],
      "metadata": {
        "id": "2cQrzLaKd9Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We can see that Survived, Pclass, SibSp and Parch have an integer data type but we know that they are categorical variables. So lets convert them to object datatype.***"
      ],
      "metadata": {
        "id": "L8eomM59f4Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert datatype to object for further analysis\n",
        "columns_to_convert = ['Survived','Pclass','SibSp','Parch']\n",
        "train_df[columns_to_convert] = train_df[columns_to_convert].astype(str)"
      ],
      "metadata": {
        "id": "kZJavT-peXJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical and numeric features\n",
        "cat_features = train_df.select_dtypes(exclude=\"number\").columns\n",
        "num_cols = train_df.select_dtypes(include=\"number\").columns\n",
        "print('Categorical Features are: ', cat_features)\n",
        "print('Numerical Features are: ', num_cols)"
      ],
      "metadata": {
        "id": "3FK2r_6zfzDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.countplot(x = 'Survived', data = train_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vpb2Pcnf6ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exploring the Data**"
      ],
      "metadata": {
        "id": "NVJn9I65Bn6I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**It's clear from the above plot that majority of the people onboarding the titanic didn't survived.**"
      ],
      "metadata": {
        "id": "-0hbxKmggCMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax=plt.subplots(2,3,figsize=(25,15))\n",
        "for i,col in enumerate(num_cols):\n",
        "    plt.suptitle(\"Visualizing Continuous Features\",size=25)\n",
        "    d = sns.distplot(train_df[col], ax=ax[i,0], kde=True)\n",
        "    d.set_title(f'Distribution Plot of {col}', loc='center', y=1.05, size=18, weight='bold',color='r')\n",
        "    b = sns.boxplot(data=train_df, x=col, ax=ax[i,1])\n",
        "    b.set_title(f'Boxplot of {col} ', loc='center', y=1.04, size=18, weight='bold',color='b')\n",
        "    s = sns.kdeplot(data = train_df, x = col, hue= 'Survived', shade= True, ax=ax[i,2], palette = 'ocean')\n",
        "    s.set_title(f'Distribution of {col} based on Survived', loc='center', y=1.04, size=18, weight='bold',color='green')\n"
      ],
      "metadata": {
        "id": "NuYQDYYkgDnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***From the above distribution plot of age based on survival we can say that children tend to have more chances of survival as compared to older individuals.***"
      ],
      "metadata": {
        "id": "j8wiiB3kgOAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking value in each categorical feature\n",
        "cat_cols = cat_features[1:]\n",
        "for col in cat_cols:\n",
        "    print(f'============{col}============\\n {train_df[col].value_counts()}\\n')"
      ],
      "metadata": {
        "id": "I4EFq89pgPXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(5,2,figsize=(18,30))\n",
        "for i, col in enumerate(cat_cols):\n",
        "    sns.countplot(data = train_df, x = col, ax=ax[i,0])\n",
        "    sns.countplot(data = train_df, x = col,hue='Survived', ax=ax[i,1])\n",
        "    if i == 0:\n",
        "        ax[0,0].set_title('Count plot for Categorical Features', loc='center', y=1.1, size=18, weight='bold',color='green')\n",
        "    else:\n",
        "        ax[0,1].set_title('Count plot for Categorical Features Based on Survived', loc='center', y=1.1, size=18, weight='bold',color='green')"
      ],
      "metadata": {
        "id": "hKUCKy2sg07a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1. It seems like people travelling in 3rd class were less likely to survive as compared to people travelling in first class.***\n",
        "\n",
        "***2. We can also conclude that females passengers were more likely to survive as compared to males.***\n",
        "\n",
        "***3. Majority of the people Embarked from Southampton, so I have decided to fill the missing values in Embarked column with S(Southampton).***"
      ],
      "metadata": {
        "id": "QWjLXFLdg9G6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ],
      "metadata": {
        "id": "q0OqLcA-ihLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Both the SibSp and Parch column suggests whether the person was person was travelling with his family or not. So we will convert these features into a single feature called SibSP_Parch.**"
      ],
      "metadata": {
        "id": "OyqfIhQIioFr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert datatype to object for further analysis\n",
        "columns_to_convert = ['Survived','Pclass','SibSp','Parch']\n",
        "train_df[columns_to_convert] = train_df[columns_to_convert].astype(int)"
      ],
      "metadata": {
        "id": "N5dUPw_aiuvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['SibSP_Parch'] = np.where(train_df['SibSp'] + train_df['Parch'] > 0, 1, 0)\n",
        "# drop SibSp and Parch\n",
        "train_df.drop(['SibSp', 'Parch'], axis= 1, inplace= True)"
      ],
      "metadata": {
        "id": "JalNwTTLisHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_encoding_features = ['Sex','Embarked']\n",
        "scaling_features = num_cols"
      ],
      "metadata": {
        "id": "A7PIDb8vorJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the data into X and y\n",
        "X = train_df.drop('Survived', axis=1)\n",
        "y = train_df['Survived']"
      ],
      "metadata": {
        "id": "B-5b2dThotgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying first 5 data of X**"
      ],
      "metadata": {
        "id": "zilXNXRqo5KW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "45TMleXpo0vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For categorical features, we'll impute the missing values with the mode of the column and encode them with One-Hot encoding***"
      ],
      "metadata": {
        "id": "dW96VkumpLGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "categorical_pipeline = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy='most_frequent')),\n",
        "        (\"onehot-encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False, drop='first')),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "ZoJHAmm-pDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***For the numeric features, specifically 'Age' we will impute the missing values with mean of the column***"
      ],
      "metadata": {
        "id": "rlAf4TJopRfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_pipeline = Pipeline(\n",
        "    steps=[\n",
        "           (\"imputer\", SimpleImputer(strategy='median')),\n",
        "            (\"scaler\", StandardScaler())\n",
        "         ]\n",
        ")"
      ],
      "metadata": {
        "id": "oiomyraRpTR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Next, we will input these along with their corresponding pipelines into a ColumnTransFormer instance***"
      ],
      "metadata": {
        "id": "QThIc9HBpWy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_transformer  = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numeric\", numeric_pipeline, scaling_features),\n",
        "        (\"categorical\", categorical_pipeline, onehot_encoding_features),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "aa113o9Ypaee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apply preprocessing***"
      ],
      "metadata": {
        "id": "msKGkE8vpeox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed  = col_transformer.fit_transform(X)\n",
        "y_transformed = y.values.reshape(-1,1)\n",
        "print('X Shape: ', X_transformed.shape)\n",
        "print('y shape: ', y_transformed.shape)"
      ],
      "metadata": {
        "id": "uHDyAYsNphj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onehot_cols = (\n",
        "    col_transformer\n",
        "    .named_transformers_[\"categorical\"]\n",
        "    .named_steps[\"onehot-encoder\"]\n",
        "    .get_feature_names_out(onehot_encoding_features)\n",
        ")\n",
        "onehot_cols"
      ],
      "metadata": {
        "id": "Gki0Rz8UpntW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "passthrough_features = [col for col in X.columns if (col not in onehot_encoding_features) and (col not in scaling_features)]\n",
        "transformed_columns = scaling_features.tolist() + onehot_cols.tolist() + passthrough_features\n",
        "transformed_columns"
      ],
      "metadata": {
        "id": "uw0HKYbTpqp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed = pd.DataFrame(X_transformed, columns = transformed_columns)\n",
        "X_transformed.head()"
      ],
      "metadata": {
        "id": "n_aGWKG4pr4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now we will split train and test data***"
      ],
      "metadata": {
        "id": "YkA8Uyhhp3F9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_transformed, y_transformed, test_size = 0.2, random_state = 2022)"
      ],
      "metadata": {
        "id": "bhde9B-Gp5NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Choosing the Model**"
      ],
      "metadata": {
        "id": "vdb3VdHIB-go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Selecting my classification model (trying  7 model to find best one)**"
      ],
      "metadata": {
        "id": "ByI9TUH-qPxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will implement Logistic Regression, KNeighborsclassifier, SVC (Support Vector Classifier), GaussianNB, DecisionTreeClassifier, RandomForestClassifier, XGBClassifier models."
      ],
      "metadata": {
        "id": "L7E-5vt-GC3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Models\n",
        "models = [\n",
        "           LogisticRegression(solver='liblinear'),\n",
        "           KNeighborsClassifier(n_neighbors = 5),\n",
        "           SVC(probability=True),\n",
        "           GaussianNB(),\n",
        "           DecisionTreeClassifier(random_state=2022),\n",
        "           RandomForestClassifier(random_state=2022),\n",
        "           XGBClassifier(random_state=2022)]\n",
        "\n",
        "model_names=['Logistic Regression','KNN', 'SVM','Naive Bayes', 'Decision Tree','Random Forest','XGBoost']"
      ],
      "metadata": {
        "id": "oUAIVzbFxV5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Put the ROC, AUC scores and accuracy scores in a data frame. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. AUC stands for \"Area under the ROC Curve.\" AUC provides an aggregate measure of performance across all possible classification thresholds.***\n"
      ],
      "metadata": {
        "id": "b85TolYqx0_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_models(models, model_names):\n",
        "    # lets create an empty lists to append the results\n",
        "    roc_auc_scores  = []\n",
        "    accuracy_scores = []\n",
        "    results = {}\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(15,10))\n",
        "    axes = axes.ravel()\n",
        "    fig.delaxes(axes[-1])\n",
        "    # use enumerate() and zip() function to iterate the lists\n",
        "    for idx, (ml_model_names, ml_models, ax) in enumerate(zip(model_names, models, axes.flatten())):\n",
        "        clf = models[idx]\n",
        "        clf.fit(X_train,y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "        plot_confusion_matrix(clf,\n",
        "                              X_test,\n",
        "                              y_test,\n",
        "                              ax=ax,\n",
        "                              cmap='Blues')\n",
        "        ax.title.set_text(ml_model_names)\n",
        "\n",
        "        print(\"Model: {}\".format(ml_model_names))\n",
        "        print(\"Accuracy: {}\".format(accuracy))\n",
        "        print(\"Roc Auc Score: {}\".format(roc_auc))\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "        roc_auc_scores.append(roc_auc)\n",
        "        accuracy_scores.append(accuracy)\n",
        "\n",
        "    results = {'Model':model_names,\n",
        "           'ROC AUC Score':roc_auc_scores,\n",
        "           'Accuracy Score':accuracy_scores}\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    # Put the roc_auc_scores and accuracy scores in a data frame.\n",
        "    models_scores_df = pd.DataFrame(results)\n",
        "    return models_scores_df"
      ],
      "metadata": {
        "id": "40lTE7TEx1_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_scores_df = build_models(models, model_names)"
      ],
      "metadata": {
        "id": "xBY8-hT9x5zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Above we can see Model Name Along accuracy and ROC AUC score.We also can create Confusion Matrix for our models .***"
      ],
      "metadata": {
        "id": "QJfa4yMGzOzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy measures how many observations, both positive and negative, were correctly classified. We shouldnâ€™t use accuracy on imbalanced problems. Then, it is easy to get a high accuracy score by simply classifying all observations as the majority class.**\n"
      ],
      "metadata": {
        "id": "F3nMhxIDzueg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We should use ROC AUC score when we care equally about positive and negative classes. It naturally extends the imbalanced data discussion from the last section. If we care about true negatives as much as we care about true positives then it totally makes sense to use ROC AUC.**"
      ],
      "metadata": {
        "id": "IWjwBIIm0K8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now we will create a comparison between all models:***"
      ],
      "metadata": {
        "id": "IQv20g3S0R2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_scores_df"
      ],
      "metadata": {
        "id": "mJp9ftkH0mlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above we will choose SVM (Support Vector Machine) as our final model, as we can see SVM has highest accuracy and very good ROC AUC score.**"
      ],
      "metadata": {
        "id": "DyMAVbzP0pbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# final model\n",
        "svc = SVC(random_state=2022,verbose=0)\n",
        "svc.fit(X_transformed, y_transformed)"
      ],
      "metadata": {
        "id": "l_0z7gBN1IpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prediction on Test Data**"
      ],
      "metadata": {
        "id": "s2k3aDt01hkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The algorithm will generate probable values for an unknown variable for each record in the new data, allowing the model builder to identify what that value will most likely be**."
      ],
      "metadata": {
        "id": "Tml5dMetFNiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# passenger ids\n",
        "test_ids = test_df['PassengerId']"
      ],
      "metadata": {
        "id": "d4dVIJ7U1jVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature engineering\n",
        "test_df['SibSP_Parch'] = np.where(test_df['SibSp'] + test_df['Parch'] > 0, 1, 0)"
      ],
      "metadata": {
        "id": "bshAzCIZ10Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert datatype to object\n",
        "test_columns_to_convert = ['Pclass','SibSP_Parch']\n",
        "test_df[test_columns_to_convert] = test_df[test_columns_to_convert].astype(str)"
      ],
      "metadata": {
        "id": "_79PW0fd13Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test_df[X.columns]\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "krvzU9j715rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# categorical and numeric features\n",
        "test_cat_features = test_data.select_dtypes(exclude=\"number\").columns\n",
        "test_num_cols = test_data.select_dtypes(include=\"number\").columns\n",
        "print('Test Data Categorical Features are: ', test_cat_features)\n",
        "print('Test Data Numerical Features are: ', test_num_cols)"
      ],
      "metadata": {
        "id": "MszwxNRx17M4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Next, we will input these along with their corresponding pipelines into a ColumnTransFormer instance.**"
      ],
      "metadata": {
        "id": "_WiruTuz2GKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_col_transformer  = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"numeric\", numeric_pipeline, test_num_cols),\n",
        "        (\"categorical\", categorical_pipeline, onehot_encoding_features),\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "6NhW09lm2H7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Apply preprocessing on test data***"
      ],
      "metadata": {
        "id": "lQzXHG2K2OTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing\n",
        "X_transformed_test  = test_col_transformer.fit_transform(test_data)\n",
        "print('Test Data Shape: ', X_transformed_test.shape)"
      ],
      "metadata": {
        "id": "pN70FYDq2Myw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_transformed_test = pd.DataFrame(X_transformed_test, columns = transformed_columns)\n",
        "X_transformed_test.head()"
      ],
      "metadata": {
        "id": "BDmTFzng2TEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_pred = svc.predict(X_transformed_test)"
      ],
      "metadata": {
        "id": "90Q9TnPV4kAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test_ids,\n",
        "    'Survived': final_pred\n",
        "})"
      ],
      "metadata": {
        "id": "3CGrk6O644YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Submission Data shape :**"
      ],
      "metadata": {
        "id": "-rjbAtX55HVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "submission.shape"
      ],
      "metadata": {
        "id": "6d29qpoa5L0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "7x4snwCM5-29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.Survived.value_counts()"
      ],
      "metadata": {
        "id": "wimz5d7g49j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('submission.csv', index= False)"
      ],
      "metadata": {
        "id": "y5yxDkwA5UF1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}